{"cells":[{"cell_type":"markdown","id":"2a024ec3","metadata":{"id":"2a024ec3"},"source":["# Redes neuronales recursivas (RNNs) para análisis de opinión.\n","\n","### Procesamiento del Lenguaje Natural\n","\n","\n","------------------------------------------------------\n","\n","\n","### Data Science and Machine Learning\n","\n","#### Febrero 2023\n","\n","**Aurora Cobo Aguilera**\n","\n","**The Valley**\n","\n","------------------------------------------------------\n","\n","En este notebook vamos a comparar un modelo basado en word embeddings-RNN-MLP con técnicas ya conocidas de clasificación de documentos:\n","\n","1) TF-IDF + regresión logística\n","\n","2) Promedio de word embeddings + k-NN\n","\n","Además, para mostrar que el cuello de botella está en la representación del texto y no tanto en el clasificador, mostraremos también cómo un clasificador de tipo MLP no es capaz de mejorar las dos soluciones anteriores. Como veremos, el procesamiento secuencial de una RNN es clave. \n","\n","Utilizaremos [Pytorch](https://pytorch.org/) para construir y entrenar la distintas redes neuronales. Para una introducción detallada de Pytorch, se recomiendan los siguientes tutoriales:\n","\n","* [Pytorch official website](https://pytorch.org/tutorials/beginner/basics/intro.html)\n","\n","* [University of Amsterdam (UvA) Deep Learning Tutorials](https://uvadlc-notebooks.readthedocs.io/en/latest/index.html)\n","\n","\n","\n"]},{"cell_type":"markdown","source":["# Pipeline para el procesamiento de texto \n","\n","Como sabemos, los algoritmos de ML procesan números, no palabras, por lo que necesitamos transformar el texto en números significativos que contengan la información relevante de los documentos. Este proceso de conversión de texto a números es lo que llamaremos **vectorización**. \n","\n","No obstante, para tener una representación útil, se requieren normalmente algunos pasos de **preprocesamiento** previo que limpien y homogeneicen los documentos: tokenización, eliminación de *stop-words*, lematización, etc.\n","La siguiente figura muestra los diferentes pasos que debemos seguir para procesar nuestros documentos hasta poder ser utilizados por nuestro modelo de aprendizaje:\n","\n","<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/BBVA/NLP/PipelineNLP.png\" width=\"80%\"> \n","\n","A lo largo de este notebook, veremos algunas de las herramientas que tenemos disponibles en Python para llevar a cabo todos estos pasos previos al despliegue de los modelos de aprendizaje automático o estadístico. Las librerías más conocidas para NLP son las siguientes:\n","* [NLTK, Natural Language ToolKit](https://www.nltk.org/). Esta libreria es una excelente biblioteca de NLP escrita en Python por expertos tanto del mundo académico como de la industria. NLTK permite crear aplicaciones con datos textuales rápidamente, ya que proporciona un conjunto de clases básicas para trabajar con corpus de datos, incluyendo colecciones de textos (corpus), listas de palabras clave, clases para representar y operar con datos de tipo texto (documentos, frases, palabras, ...) y funciones para realizar tareas comunes de NLP (conversión a token, conteo de palabras, ...). NLTK va a ser de gran ayuda para el preprocesamiento de los documentos.\n","\n","\n","* [Gensim](https://pypi.org/project/gensim/) es otra librería de Python para la realización de modelado por temáticas (*topic modeling*), la indexación de documentos y tareas de recuperación de la información a partir de documentos. Está diseñada para operar con grandes cantidades de información (con implementaciones eficientes y paralelizables/distribuidas) y nos va a ser de gran ayuda para la vectorización de nuestros corpus de datos una vez preprocesados.\n","\n","* [Sklearn](https://scikit-learn.org/stable/index.html). Sklearn es una librería destinada principalmente al diseño de modelos de aprendizaje automático para clasificación y regresión, pero también incluye algunas fucionalidades para el preprocesamiento de textos.\n","\n","Además existen otras librerías como [Spacy](https://spacy.io/), [CoreNLP](https://stanfordnlp.github.io/CoreNLP/), [Hugging Face](https://huggingface.co/transformers/) o paquetes incluidos en Pytorch, Tensorflow que nos permiten realizar parte de estas tareas."],"metadata":{"id":"3-gvyN7hSXne"},"id":"3-gvyN7hSXne"},{"cell_type":"markdown","source":["\n","## 1. Datos de texto para clasificación\n","Como base de datos usaremos la base de datos [Finantial Phrase Bank](https://www.researchgate.net/profile/Pekka-Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip) contiene cerca de 5000 oraciones extraídas de textos de noticias financieras:\n","\n","\n",">*This release of the financial phrase bank covers a collection of 4840 sentences. The selected collection of phrases was annotated by 16 people with adequate background knowledge on financial markets. Three of the annotators were researchers and the remaining 13 annotators were master’s students at Aalto University School of Business with majors primarily in finance, accounting, and economics.*\n",">\n",">*The objective of the phrase level annotation task was to classify each example sentence into a positive, negative or neutral category by considering only the information explicitly available in the given sentence. Since the study is focused only on financial and economic domains, the annotators were asked to consider the sentences from the view point of an investor only; i.e. whether the news may have positive, negative or neutral influence on the stock price. As a result, sentences which have a sentiment that is not relevant from an economic or financial perspective are considered neutral.*\n","\n","Vamos a cargar la base de datos y procederemos al pre-procesado.\n","\n"],"metadata":{"id":"STZPemxQO9iF"},"id":"STZPemxQO9iF"},{"cell_type":"code","execution_count":null,"id":"2bc2990f","metadata":{"id":"2bc2990f"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch \n","import spacy\n","\n","import gensim\n","from gensim.models import TfidfModel\n","from gensim.matutils import corpus2dense, corpus2csc\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina' \n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"274c6141","metadata":{"id":"274c6141"},"outputs":[],"source":["with open(\"Sentences.txt\", \"r\", encoding=\"ISO-8859-1\") as sentences:\n","    lines = sentences.readlines()"]},{"cell_type":"code","execution_count":null,"id":"64954571","metadata":{"id":"64954571"},"outputs":[],"source":["print(lines[0])"]},{"cell_type":"markdown","id":"e6604b9d","metadata":{"id":"e6604b9d"},"source":["Codificamos las etiquetas con 0 (neutra), 1 (negativa) y 2 (positiva)"]},{"cell_type":"code","execution_count":null,"id":"3b870fa6","metadata":{"id":"3b870fa6"},"outputs":[],"source":["phrases = [l.split('@')[0] for l in lines]\n","opinions = [l.split('@')[1] for l in lines]\n","\n","\n","def code_opinion(l):\n","    \n","    d = 0\n","    \n","    if (l=='negative\\n'):\n","        \n","        d = 1\n","        \n","    elif (l=='positive\\n'):\n","        \n","        d = 2\n","        \n","    return d\n","\n","labels = np.array([code_opinion(l) for l in opinions])\n","    "]},{"cell_type":"code","execution_count":null,"id":"e9c548c4","metadata":{"id":"e9c548c4"},"outputs":[],"source":["df = pd.DataFrame({\"Phrase\":phrases,\n","                  \"Opinion\":opinions})\n","\n","df"]},{"cell_type":"code","execution_count":null,"id":"ecbcfc2e","metadata":{"id":"ecbcfc2e"},"outputs":[],"source":["fig,ax = plt.subplots(1,1)\n","\n","ax.hist(labels)\n","ax.set_xticks([0,1,2])\n","ax.set_xticklabels(['Neutral','Negative','Positive'],rotation=45)\n","ax.grid()\n"]},{"cell_type":"markdown","source":["## 2. Preprocesado del corpus\n","\n","Antes de transformar los datos de entrada de texto en una representación vectorial, necesitamos estructurar y limpiar el texto, y conservar toda la información que permita capturar el contenido semántico del corpus. Normalmente, se obtiene un vector por cada texto, pero dependiendo de la aplicación podría interesar obtener un vector por cada frase, por cada párrafo o, incluso, por cada palabra.\n","\n","Para ello, el procesado típico de NLP aplica los siguientes pasos:\n","\n","1. Tokenización\n","2. Limpieza\n","3. Homogeneización\n","\n","Nótese que, aunque aquí definamos esta secuencia de pasos, dependiendo de cómo se apliquen su orden puede variar el resultado."],"metadata":{"id":"rKkoJTQfTGnV"},"id":"rKkoJTQfTGnV"},{"cell_type":"markdown","source":["**Tokenización** es el proceso de dividir el texto dado en piezas más pequeñas llamadas tokens. Las palabras, los números, los signos de puntuación y otros pueden ser considerados como tokens."],"metadata":{"id":"KW4PzbzBTtAJ"},"id":"KW4PzbzBTtAJ"},{"cell_type":"markdown","source":["**Limpieza y homogeneización**\n","\n","Si observamos los tokens de un corpus podemos ver que hay muchos tokens con algunas letras en mayúsculas y otras en minúsculas, el mismo token unas veces aparece en singular y otras en plural, o el mismo verbo que aparece en diferentes tiempos verbales. Para analizar semánticamente el texto, nos interesa  **homogeneizar** las palabras que formalmente son diferentes pero tienen el mismo significado. En este proceso, lógicamente, estamos perdiendo información de estilo, matices o la intención del escritor u otros aspectos; sin embargo, muchas veces lo que nos importa es el contenido (temática) del texto y de este modo lo vamos a reforzar para la posterior vectorización.\n","\n","Para ello podemos usar las herramientas de lematización. El proceso habitual de homogeneización consiste en los siguientes pasos:\n","\n","1. Eliminación de mayúsculas y caracteres no alfanuméricos: de este modo los caracteres alfabéticos en mayúsculas se transformarán en sus correspondientes caracteres en minúsculas y  se eliminarán los caracteres no alfanuméricos, por ejemplo, los signos de puntuación.\n","\n","2. Limpieza: este paso del preprocesado consiste en eliminar las palabras irrelevantes o **stop words** de los documentos. \n","\n","\n","3. Stemming/Lematización: eliminar las terminaciones de las palabras para preservar la raíz de las palabras e ignorar la información gramatical (eliminamos marcas de plurales, género, conjugaciones verbales, ...).\n","\n","En este paso, podemos detectar typos, *misspellings*, faltas de ortografía, etc. que podríamos corregir.\n","\n"],"metadata":{"id":"76ao4CDaTzkh"},"id":"76ao4CDaTzkh"},{"cell_type":"markdown","source":["### 2.2. Preprocesado de texto con spaCy\n","\n","[spaCy](https://spacy.io/) es una biblioteca gratuita de código abierto para el procesamiento avanzado del lenguaje natural en Python. spaCy está diseñado específicamente para uso en producción. A diferencia de NLTK, spaCy sigue una orientación de objetos. Por ejemplo, cuando tokenizamos un texto, cada token es un objeto con atributos y propiedades específicas.\n","\n","Spacy da soporte para más de 64 idiomas, incluyendo modelos estadísticos ya entrenados para [17 de ellos](https://spacy.io/usage/models) (incluyendo word embeddings y modelos basados en [transformers](https://spacy.io/usage/v3), la última revolución en NLP).\n","\n","\n","Puesto que en esta sesión sólo vamos a cubrir algunos aspectos básicos de spaCy, en los siguientes recursos podéis encontrar material adicional:\n","\n","- [spaCy 101 course](https://spacy.io/usage/spacy-101)\n","- [Advanced Tutorial](https://course.spacy.io/en/)\n","\n","\n","El procesado de texto con spaCy es sencillo. Cargaremos un modelo pre-entrenado para un determinado idioma, y pasamos cualquier texto a procesar. spaCy ejecutará una serie de procesos (pipeline) sobre el mismo y  devolverá un objeto tipo `doc`.\n","\n","<figure>\n","<center>\n","<img src='https://spacy.io/images/pipeline.svg' width=\"800\"></img>\n","<figcaption>Source: https://spacy.io/images/pipeline.svg</figcaption></center>\n","</figure>\n","\n","La arquitectura básica en spaCy es la siguiente:\n","\n","   - `Language`: se determina al cargar el modelo y el pipeline de procesos asociados. Trasforma texto en objectos spaCy.\n","   - `Doc`: Secuencia iterable de tokens. \n","   - `Vocab`: Diccionario asociado al modelo.  "],"metadata":{"id":"lOoYkg2aJuqu"},"id":"lOoYkg2aJuqu"},{"cell_type":"code","source":["!pip install --upgrade spacy"],"metadata":{"id":"ICBjePdoJBpF"},"id":"ICBjePdoJBpF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m spacy download en_core_web_lg"],"metadata":{"id":"_uCBLp2BIP2O"},"id":"_uCBLp2BIP2O","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"ef1566b5","metadata":{"tags":[],"id":"ef1566b5"},"outputs":[],"source":["nlp = spacy.load(\"en_core_web_lg\")\n","\n","# Desabilitamos procesos que no vamos a utilizar\n","\n","#list_processes = [\"lemmatizer\",\"tagger\",\"parser\",\"ner\",\"attribute_ruler\"]\n","\n","#for l in list_processes:\n","#    nlp.disable_pipe(l) \n","    \n"]},{"cell_type":"code","source":["lista_vocab = list(nlp.vocab.strings)\n","\n","print(\"El tamaño del diccionario es de {} palabras\".format(len(lista_vocab)))\n","\n","# Las 200 primeras\n","print(lista_vocab[:200])"],"metadata":{"id":"2SPVk7e5MadB"},"id":"2SPVk7e5MadB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Podemos ver que el vocabulario incluye ya emoticonos codificados en modo texto.\n"],"metadata":{"id":"lRhJWABcMimb"},"id":"lRhJWABcMimb"},{"cell_type":"markdown","source":["> **Ejercicio**: Cree una lista con las palabras del vocabulario que únicamente contienen caracteres alfabéticos ([a-zA-Z]). ¿Cuántas palabras obtiene?. Imprima las 20 primeras ..."],"metadata":{"id":"UiAUHrT3MmF_"},"id":"UiAUHrT3MmF_"},{"cell_type":"code","source":["#<SOL>\n","\n","#</SOL>"],"metadata":{"id":"gJdRASvQMp68"},"id":"gJdRASvQMp68","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A continuación, vamos a utilizar el pipeline que hemos cargado para analizar un texto ..."],"metadata":{"id":"-vScPq9HMpiN"},"id":"-vScPq9HMpiN"},{"cell_type":"code","source":["print(phrases[0])\n","\n","doc = nlp(phrases[0])"],"metadata":{"id":"ENwPtnbHNUCh"},"id":"ENwPtnbHNUCh","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`doc` es un objeto iterable, compuesto por objetos tipo [`token`](https://spacy.io/api/token). En el siguiente bucle imprimimos algunas de las propiedades de dichos tokens determinadas por el pipeline que hemos cargado.\n"],"metadata":{"id":"1kEBfznrNe7T"},"id":"1kEBfznrNe7T"},{"cell_type":"code","source":["for token in doc:\n","    print(token.text, token.lemma_, token.tag_, token.is_alpha, token.is_stop,token.is_punct)\n","    print('*****')"],"metadata":{"id":"D3YAkLBqNda4"},"id":"D3YAkLBqNda4","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pre-procesamos con spaCy el texto ..."],"metadata":{"id":"adfReXqMV-Ts"},"id":"adfReXqMV-Ts"},{"cell_type":"code","source":["docs = [nlp(c) for c in phrases]\n","\n","def preprocess(doc):\n","    \n","    return [w for w in doc if not w.is_stop and w.has_vector and not w.is_punct]\n","\n","# eliminamos stopping words, puntuaciones y tokens sin word embedding\n","norm_docs = [preprocess(d) for d in docs]\n","\n","# Pasamos a modo texto\n","norm_docs_text = [[w.text.lower() for w in d] for d in norm_docs]"],"metadata":{"id":"reCZwAw7V4kU"},"id":"reCZwAw7V4kU","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Vectorización\n","\n","Un *embedding* es una representación vectorial de nuestros datos en un espacio de dimensión **relativamente bajo**. La representación mediante *embeddings* suele utilizarse para facilitar el aprendizaje de modelos cuando se tienen que manejar datos categóricos o conceptos cuya codificación suele llevar a representaciones *sparse* de alta dimensión, como pueden ser codificaciones one-hot encoding de variables categóricas estandar o  representaciones BoW o TF-IDF de documentos. \n","\n","Así, por ejemplo, una codificación one-hot de paises y ciudades nos llevaría a vectores de este tipo:\n","\n","<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/BBVA/Embeddings/OneHotEncoding.png\" width=\"40%\"> \n","\n","\n","Al utilizar este tipo de representaciones en un sistema de aprendizaje automático nos encontramos varias limitaciones:\n","* Por un lado, trabajar con un espacio de **muy alta dimensión** implica entrenar un modelo con muchos parámetros, lo que a su vez conlleva un mayor coste computacional, riesgo de sobreajuste, ...\n","* Por otro lado, el tener que manejar representaciones dispersas dificulta el cálculo de distancias entre elementos así como la **identificación de elementos similares**. Por ejemplo, la codificación anterior nos va a dar la misma distancia entre Rome y Paris que entre Rome e Italia.\n","\n","Por tanto, para utilizar este tipo de representaciones en un sistema de aprendizaje automático, necesitamos una forma de representar cada vector disperso como un vector de números para el que los elementos semánticamente similares (ciudades, películas o palabras) tengan distancias similares en el espacio vectorial. La solución a estos problemas es utilizar *embeddings*, ya que son capaces de transformar grandes vectores dispersos a un espacio de menor dimensión que preserva las relaciones semánticas. \n","\n","Idealmente, un buen *embedding* debe proporcionar un conjunto de vectores cuya posición (distancia y dirección) en el espacio vectorial codifique la semántica de los datos que representan. Las siguientes visualizaciones$^{(*)}$ de *embeddings* reales muestran relaciones geométricas que capturan relaciones semánticas como el genero, un tiempo verbal o la relación entre un país y su capital\n","\n","<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/BBVA/Embeddings/Embeddings1.svg\" width=\"80%\"> \n","\n","Además, la representación de los datos mediante  *embeddings* tiene varias ventajas:\n","* Ayuda al aprendizaje del modelo, ya que reducirá el coste computacional y evitará problemas de sobreajuste.  \n","* Esta representación puede aprenderse y reutilizarse en distintos modelos.\n","* Al manejar los datos en un espacio de menor dimensión se facilita su representación, ya sea porque el *embedding* nos permite tener los datos en un espacio de dos o tres dimensiones, o porque se puede combinar con algoritmos de visualización (que veremos más adelante) que nos permiten visualizar lo que está ocurriendo en este espacio del *embedding*.\n","\n","$^{(*)}$ https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space\n"],"metadata":{"id":"-PLNcpwXOtmg"},"id":"-PLNcpwXOtmg"},{"cell_type":"markdown","source":["### 3.1 Representación TF-IDF \n","\n","Un problema del BoW es que la frecuencia de las palabras muy frecuentes empiezan a dominar en el documento respecto al resto; por ejemplo, verbos muy comunes o términos habituales en el contexto del corpus pero no tienen tanto \"contenido informativo\" para el modelo como pueden ser palabras más raras pero tal vez específicas del dominio. Al realizar la gestión del vocabulario podríamos eliminar estas palabras tan frecuentes, pero este proceso es bastante manual y es preferible tener una vectorización robusta a esto.\n","\n","Para ello, la representación TF-IDF (Term Frequency–Inverse Document Frequency) propone reajustar la frecuencia de las palabras en función de la frecuencia con que aparecen en todos los documentos, de modo que se penalicen las puntuaciones de palabras frecuentes si también son frecuentes en todos los documentos. Para ello, el TF-IDF implica el cálculo de dos valores:\n","\n","\n","**Frecuencia de término (TF)**\n","\n","Por frecuencia de término $\\text{TF}(w)$ nos referimos al número de veces que una palabra $w$ dada ocurre en un documento, dividido por el número total de palabras en dicho documento.\n","$$ \\text{TF}(w,d) =\\frac{\\text{# veces que $w$ aparece en el documento $d$}}{\\text{# total de palabras en el documento $d$}}$$\n","\n","**Frecuencia de Documento Inversa (IDF)**\n","\n","Es una medida de cuánta información proporciona la palabra $w$, es decir, si es común o rara en todos los documentos del corpus $D$. Se calcula de la siguiente manera:\n","$$ \\text{IDF}(w,D) =\\log \\frac{\\text{# palabras en el corpus}}{1+\\text{# documentos donde la palabra $w$ aparece}}$$\n"," \n","A partir de estos valores el **TF-IDF** se calcula de la siguiente manera:\n","\n","$$\\text{TF-IDF}(w,d,D) = \\text{TF}(w,d) * \\text{IDF}(w,D)$$\n","\n","Un peso alto en TF-IDF se consigue cuando la palabra tiene una frecuencia alta en el documento y, a la vez, una frecuencia baja en el corpus; por lo tanto, los pesos tienden a filtrar los términos que son comunes a muchos documentos. \n"],"metadata":{"id":"zula7q0vU4dn"},"id":"zula7q0vU4dn"},{"cell_type":"markdown","source":["Aplicamos el TF-IDF con filtrado básico del diccionario ..."],"metadata":{"id":"Mkl_P8zPWFrl"},"id":"Mkl_P8zPWFrl"},{"cell_type":"code","source":["# Diccionario y TF-IDF\n","\n","D = gensim.corpora.Dictionary(norm_docs_text)\n","\n","no_below = 10 # Mínimo número de documentos para mantener un término en el diccionario\n","no_above = .8 # Máxima proporción de docuemntos en los cuales un término puede aparecer para mantenerlo en el diccionario\n","\n","D.filter_extremes(no_below=no_below,no_above=no_above)\n","\n","corpus_bow = [D.doc2bow(doc) for doc in norm_docs_text]\n","\n","model = TfidfModel(corpus_bow)  \n","\n","corpus_tfidf = model[corpus_bow]\n","\n","n_tokens = len(D)\n","num_docs = len(corpus_bow)\n","\n","# Convertir a representación TF-IDF \n","C = corpus2dense(corpus_tfidf, num_terms=n_tokens, num_docs=num_docs).T\n","\n","print(f\"La dimensión de la matriz TF-IDF es {C.shape[0]} x {C.shape[1]}\")"],"metadata":{"id":"09KBvdLmWAlR"},"id":"09KBvdLmWAlR","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2 Representación Word2Vec\n","\n","[Word2vec](https://arxiv.org/pdf/1310.4546.pdf) utiliza un tipo muy sencillo de **red neuronal** para aprender asociaciones de palabras de un gran corpus de texto. Word2vec se basa en la premisa de que palabras con significado similar deben ser codificadas con vectores geométricamente cercanos. Para ello se considera que las palabras que suelen tener las mismas palabras vecinas (contexto) tienden a ser semánticamente similares. Tanto \"perro\" como \"gato\" aparecen con frecuencia cerca de la palabra \"veterinario\", y este hecho refleja su similitud semántica. "],"metadata":{"id":"48GYjSIWRYcd"},"id":"48GYjSIWRYcd"},{"cell_type":"code","execution_count":null,"id":"3e388290","metadata":{"id":"3e388290"},"outputs":[],"source":["# Volvemos a crear una lista de objetos spacy.Doc y extraemos la media de los word embeddings ...\n","W = np.array([nlp(' '.join(d)).vector for d in norm_docs_text])\n","\n","print(f\"La dimensión de la matriz de embeddings es {W.shape[0]} x {W.shape[1]}\")\n"]},{"cell_type":"markdown","source":["Ya tenemos las representaciones vectoriales de mi base de datos de texto, ahora puedo proceder como hemos estudiado hasta ahora y aplicar modelos de clasificación con ScikitLearn."],"metadata":{"id":"LdrTh1aJWKpH"},"id":"LdrTh1aJWKpH"},{"cell_type":"markdown","source":["## 4. Clasificación binaria\n"],"metadata":{"id":"l7iUpcZxYeyD"},"id":"l7iUpcZxYeyD"},{"cell_type":"markdown","source":["Primero vamos a binarizar mis etiquetas y crear los conjuntos de entrenamiento, validación y test."],"metadata":{"id":"iw-hhSmeW_0L"},"id":"iw-hhSmeW_0L"},{"cell_type":"code","execution_count":null,"id":"569b2e6b","metadata":{"id":"569b2e6b"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","labels_bin = labels.copy()\n","\n","labels_bin[labels_bin==2] = 0 \n","\n","idx_data = np.arange(0,C.shape[0],1)\n","\n","# Separamos train de test\n","idx_train, idx_test, y_train, y_test = train_test_split(idx_data, labels_bin, test_size=0.2, random_state=0)\n","\n","# Separamos train de val\n","idx_train, idx_val, y_train, y_val = train_test_split(idx_train, y_train, test_size=0.2, random_state=0)\n","\n","acc_baseline_train = np.sum(y_train==0)/y_train.shape[0]\n","\n","print(f\"El % de etiquetas correctas del clasificador baseline es {acc_baseline_train}\")"]},{"cell_type":"markdown","id":"6be323cd","metadata":{"id":"6be323cd"},"source":["Obtenidas las dos representaciones vectoriales por documento, vamos a entrenar un RL **binario** para clasificar cada oración entre negativa (1) y neutral/positiva (0). Observe que la base de datos está muy desbalanceada."]},{"cell_type":"markdown","id":"5a9a844e","metadata":{"id":"5a9a844e"},"source":["\n","\n","\n","### 4.1 Clasificación TF-IDF + LR"]},{"cell_type":"code","execution_count":null,"id":"13631f68","metadata":{"id":"13631f68"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression as LR\n","from sklearn.model_selection import GridSearchCV\n","\n","rango_C = np.logspace(-3, 3, 20)  # Rango C en escala logarítmica (base 10). Esto es, 20 puntos desde 10^3, a 10^3.\n","diccionario_parametros = [{'C': rango_C}]   \n","nfold = 10 # Número de particiones train/validación\n","\n","\"\"\" Ajusto C por validación cruzada\n","El optimizador por defecto ('lbfgs') no acepta regularización l1. \n","Usamos 'liblinear' siguiendo las recomendaciones de la librería.\n","\"\"\"\n","LR_with_CV  = GridSearchCV(estimator=LR(penalty='l1',max_iter=1e08,solver='liblinear'),\n","                                  param_grid=diccionario_parametros,cv=nfold)\n","# Entrenar el modelo\n","LR_with_CV.fit(C[idx_train,:],y_train)   \n","\n","print(\"El mejor parámetro C es {0:.2f}\".format(LR_with_CV.best_params_['C']))\n","                        \n","# Score de claisficación en train/test\n","accuracy_train = LR_with_CV.score(C[idx_train,:],y_train)   \n","accuracy_test = LR_with_CV.score(C[idx_test,:],y_test)  \n","\n","print(\"Accuracy train {0:.2f}%. Accuracy test {1:.2f}%\\n\".format(accuracy_train*100, accuracy_test*100))\n"]},{"cell_type":"markdown","source":["> **Ejercicio**: Completa el código con las funciones que ya conoces."],"metadata":{"id":"h6GLgY9-dty1"},"id":"h6GLgY9-dty1"},{"cell_type":"markdown","id":"1890930f","metadata":{"id":"1890930f"},"source":["### 4.2 Clasificación media WE + LR"]},{"cell_type":"code","execution_count":null,"id":"9f158384","metadata":{"id":"9f158384"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","# A diferencia de TF-IDF, en este caso normalizamos las características (columnas de W)\n","\n","standarize = StandardScaler().fit(W[idx_train,:])   \n","\n","W_train = standarize.transform(#<SOL> \n","W_test = standarize.transform(#<SOL> \n","\n","\n","rango_C = np.logspace(-3, 3, 20)  # Rango C en escala logarítmica (base 10). Esto es, 20 puntos desde 10^3, a 10^3.\n","diccionario_parametros = [{'C': rango_C}]   \n","nfold = 10 # Número de particiones train/validación\n","\n","emb_LR_with_CV = GridSearchCV(estimator=LR(penalty='l2',max_iter=1e04),param_grid=diccionario_parametros,cv=nfold)\n","# Entrenar el modelo\n","emb_LR_with_CV.fit(  #<SOL> \n","\n","print(\"El mejor parámetro C es {0:.2f}\".format(emb_LR_with_CV.best_params_['C']))\n","                        \n","# Score de clasificación en train/test\n","accuracy_train =     #<SOL>\n","accuracy_test =       #<SOL>\n","\n","print(\"Accuracy train {0:.2f}%. Accuracy test {1:.2f}%\\n\".format(accuracy_train*100, accuracy_test*100))\n"]},{"cell_type":"markdown","id":"28843e77","metadata":{"id":"28843e77"},"source":["### 4.3 Clasificación media WE + k-NN\n","\n","Normalizamos por filas para emular distancia coseno ..."]},{"cell_type":"code","execution_count":null,"id":"8fd13cc3","metadata":{"id":"8fd13cc3"},"outputs":[],"source":["norms = np.linalg.norm(W,axis=1)\n","\n","norms[norms<1e-6] = 1e-6\n","\n","W_ = W/norms.reshape([-1,1])"]},{"cell_type":"code","execution_count":null,"id":"b1d6bfc6","metadata":{"id":"b1d6bfc6"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","# Entrenamiento k-NN con validación de vecinos y ponderación de distancias\n","K_max = 15\n","rango_K = np.arange(1, K_max+1)\n","nfold = 10\n","\n","# Define con el nombre de los parámetros a explorar como clave y los rangos a explorar como valores\n","diccionario_parametros = #<SOL>\n","\n","# Validación cruzada con GridSearchCV\n","knn = GridSearchCV(#<SOL>\n","# Entrenamiento\n","knn.fit(W_[idx_train, :], y_train)\n","# Test\n","accuracy_train_knn = #<SOL> \n","accuracy_test_knn = #<SOL> \n","\n","print(\"El número de vecinos seleccionado es k={0:d}\".format(knn.best_params_['n_neighbors']))\n","print(\"Accuracy train {0:.2f}%. Accuracy test {1:.2f}%\\n\".format(accuracy_train_knn*100, accuracy_test_knn*100))"]},{"cell_type":"markdown","id":"b3150c4f","metadata":{"id":"b3150c4f"},"source":["#### Representación curvas ROC y PR"]},{"cell_type":"code","execution_count":null,"id":"0e6233dd","metadata":{"id":"0e6233dd"},"outputs":[],"source":["from sklearn import metrics\n","\n","fpr, recall, thresholds = metrics.roc_curve(y_test, LR_with_CV.predict_proba(C[idx_test,:])[:,1], pos_label=1) \n","fpr2, recall2, thresholds = metrics.roc_curve(y_test, emb_LR_with_CV.predict_proba(W_test)[:,1], pos_label=1) \n","fpr3, recall3, thresholds = metrics.roc_curve(y_test, knn.predict_proba(W_[idx_test,:])[:,1], pos_label=1) \n","\n","fig,ax = plt.subplots()\n","plt.plot(fpr,recall,lw=2.5,label='Curva ROC TF-IDF')\n","plt.plot(fpr2,recall2,lw=2.5,label='Curva ROC embeddings')\n","plt.plot(fpr3,recall3,lw=2.5,label='Curva ROC embeddings con K-NN')\n","plt.legend(loc=7)\n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('Recall (R)')\n","plt.title('Curva ROC')\n","plt.show()\n","\n","area_roc_tf_idf = metrics.roc_auc_score(y_test, LR_with_CV.predict_proba(C[idx_test,:])[:,1])\n","area_roc_embbedings = metrics.roc_auc_score(y_test, emb_LR_with_CV.predict_proba(W_test)[:,1])\n","area_roc_embbedings_knn = metrics.roc_auc_score(y_test, knn.predict_proba(W_[idx_test,:])[:,1])\n","\n","print(f\"El área bajo la curva ROC de TF-IDF es {area_roc_tf_idf}\")\n","print(f\"El área bajo la curva ROC de embeddings es {area_roc_embbedings}\")\n","print(f\"El área bajo la curva ROC de embeddings con K-NN es {area_roc_embbedings_knn}\")"]},{"cell_type":"code","execution_count":null,"id":"ca9ee039","metadata":{"id":"ca9ee039"},"outputs":[],"source":["P, R, thresholds = metrics.precision_recall_curve(y_test, LR_with_CV.predict_proba(C[idx_test,:])[:,1], pos_label=1) \n","P2, R2, thresholds = metrics.precision_recall_curve(y_test, emb_LR_with_CV.predict_proba(W_test)[:,1], pos_label=1) \n","P3, R3, thresholds = metrics.precision_recall_curve(y_test, knn.predict_proba(W_[idx_test,:])[:,1], pos_label=1) \n","\n","fig,ax = plt.subplots()\n","plt.plot(R,P,lw=2.5,label='Curva PR TF-IDF')\n","plt.plot(R2,P2,lw=2.5,label='Curva PR embeddings')\n","plt.plot(R3,P3,lw=2.5,label='Curva PR embeddings K-NN')\n","plt.legend(loc=7)\n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.xlabel('Recall (R)')\n","plt.ylabel('Precision (P)')\n","plt.title('Curva PR')\n","plt.show()\n","\n","area_pr_tf_idf = metrics.average_precision_score(y_test, LR_with_CV.predict_proba(C[idx_test,:])[:,1])\n","area_pr_embbedings = metrics.average_precision_score(y_test, emb_LR_with_CV.predict_proba(W_test)[:,1])\n","area_pr_embbedings_knn = metrics.average_precision_score(y_test, knn.predict_proba(W_[idx_test,:])[:,1], pos_label=1) \n","\n","print(f\"El área bajo la curva PR de TF-IDF es {area_pr_tf_idf}\")\n","print(f\"El área bajo la curva PR de embeddings es {area_pr_embbedings}\")\n","print(f\"El área bajo la curva PR de embeddings con k-NN es {area_pr_embbedings_knn}\")"]},{"cell_type":"markdown","id":"3a532a91","metadata":{"id":"3a532a91"},"source":["Guardaremos los índices de aquellos datos de test donde este último clasificador falla ..."]},{"cell_type":"code","execution_count":null,"id":"848ff557","metadata":{"id":"848ff557"},"outputs":[],"source":["errores_k_NN = np.where(knn.predict(W_[idx_test,:])!=y_test)[0]"]},{"cell_type":"code","execution_count":null,"id":"1d38e753","metadata":{"id":"1d38e753"},"outputs":[],"source":["len(errores_k_NN)"]},{"cell_type":"markdown","id":"e1b7b099","metadata":{"id":"e1b7b099"},"source":["## 5. Clasificación media WE + MLP \n","\n","Vamos a implementar una MLP de 3 capas como clasificador. El diseño de una red neuronal involucra la selección de un gran número de hiperparámetros y la mejor de las soluciones require la validación cruzada de éstos, que es un proceso arduo y costoso computacionalmente. \n","\n","<img src='http://www.tsc.uc3m.es/~olmos/BBVA/MLP.png' width=800 />\n","\n","Los resultados que se muestran a continuación se han obtenido tras probar manualmente unas configuraciones básicas. En este sentido, hay margen para mejorar los resultados obtenidos. "]},{"cell_type":"code","execution_count":null,"id":"325ed068","metadata":{"id":"325ed068"},"outputs":[],"source":["# Cargamos librerías \n","from torch import nn\n","from torch import optim\n"]},{"cell_type":"code","execution_count":null,"id":"f74bbf6f","metadata":{"id":"f74bbf6f"},"outputs":[],"source":["'''\n","En esta clase definimos la estructura de la red, compuesta únicamente por capas densas, activaciones tipo Tanh,\n","capa dropout y la activación de clasificación final (log-softmax)\n","'''\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self,dimx,hidden1,hidden2,nlabels,prob): # Nlabels será 2 en nuestro caso\n","        super().__init__()\n","        \n","        self.output1 = nn.Linear(dimx,hidden1)\n","        \n","        self.output2 = nn.Linear(hidden1,hidden2)\n","        \n","        self.output3 = nn.Linear(hidden2,nlabels)\n","    \n","        self.activation = nn.Tanh()\n","        \n","        self.logsoftmax = nn.LogSoftmax(dim=1)    \n","        \n","        # Módulo dropout\n","        self.dropout = nn.Dropout(p=prob)\n","        \n","    def forward(self, x):\n","        # Pasar el tensor de entrada a través de cada una de las operaciones\n","        x = self.output1(x)\n","        x = self.activation(x)\n","        x = self.dropout(x) \n","        x = self.output2(x)\n","        x = self.activation(x)\n","        x = self.dropout(x) \n","        x = self.output3(x)\n","        x = self.logsoftmax(x) \n","        return x"]},{"cell_type":"markdown","id":"59008f11","metadata":{"id":"59008f11"},"source":["A continuación, extendemos la clase anterior añadiendo un método `fit` para entrenar la red. Como función de coste utilizaremos [Negative Log Likelihood Loss (NLLLoss)](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html), que combinada con la función [logsoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html) de la red neuronal, equivale a la entropía cruzada similar a la de una regresión logística multi-clase. Como optimizador, utilizamos [Adam](https://pytorch.org/docs/stable/optim.html).\n","\n","Durante el entrenamiento, guardaremos en dos listas la función de coste en el conjunto de entrenamiento (`loss_during_training`) y en el conjunto de validación (`valid_loss_during_training`)."]},{"cell_type":"code","execution_count":null,"id":"9d08bc64","metadata":{"id":"9d08bc64"},"outputs":[],"source":["class MLP_with_train(MLP):\n","    \n","    def __init__(self,dimx,hidden1,hidden2,nlabels,batch_size=100,lr=0.001,prob=0.2):\n","        \n","        super().__init__(dimx,hidden1,hidden2,nlabels,prob)  \n","        \n","        self.lr = lr #Learning Rate\n","        \n","        self.optim = optim.Adam(self.parameters(), self.lr) #Optimizador\n","        \n","        self.criterion = nn.NLLLoss() #Negative Log Likelihood Loss (NLLLoss)           \n","        \n","        # Listas para guardar la función de coste durante el entrenamiento, tanto en el set de entrenamiento como validación\n","        \n","        self.loss_during_training = [] \n","        \n","        self.valid_loss_during_training = [] \n","        \n","        self.batch_size = batch_size\n","        \n","    def fit(self,X,Y,Xval,Yval,epochs=100,print_every=5):\n","        \n","        '''\n","        - X: datos de entrenamiento. \n","        - Y: etiquetas de entrenamiento. \n","        - Xval: datos de validación\n","        - Yval: etiquetas de validación\n","        - epochs: número de épocas (cuantas veces recorremos el dataset durante el entrenamiento)\n","        - print every: impresión del coste en entrenamiento y valdiación cada print_every épocas\n","        '''\n","        \n","        self.print_every = print_every\n","        \n","        self.epochs=epochs\n","        \n","        # Bucle de optimización\n","        \n","        self.num_train = X.shape[0]\n","        \n","        self.num_batchs = np.floor(self.num_train/self.batch_size)\n","        \n","        self.num_val = Xval.shape[0]\n","        \n","        self.num_batchs_val = np.floor(self.num_val/self.batch_size)        \n","        \n","        labels = torch.Tensor(Y).type(torch.LongTensor)\n","        \n","        labelsval = torch.Tensor(Yval).type(torch.LongTensor)\n","        \n","        \n","        for e in range(int(self.epochs)):\n","            \n","            self.train() # Dropout activado\n","            \n","            # Permutación aleatoria datos  \n","            \n","            idx = np.random.permutation(self.num_train)\n","            \n","            running_loss = 0.\n","            \n","            for i in range(int(self.num_batchs)):\n","                        \n","                self.optim.zero_grad()  # Ponemos gradientes a cero\n","\n","                # Índices del siguiente mini-batch de datos\n","\n","                idx_batch = idx[i*self.batch_size:(i+1)*self.batch_size]\n","                \n","                # Salida de la red para ese mini-batch\n","\n","                out = self.forward(X[idx_batch,:])\n","                \n","                # Evaluación función de coste\n","\n","                loss = self.criterion(out,labels[idx_batch])\n","                \n","                # Guardamos su valor (para visualizar)\n","\n","                running_loss += loss.item()\n","                \n","                # Calculamos gradientes\n","\n","                loss.backward()\n","                \n","                # Iteración del optimizador\n","                \n","                self.optim.step()\n","                \n","            self.loss_during_training.append(running_loss/self.num_batchs)\n","            \n","            # Repetimos para el conjunto de validación (pero sin calcular gradientes\n","            # solo monitorizamos función de coste para early stopping)\n","            \n","            with torch.no_grad(): \n","                \n","                \n","                self.eval() # Dropout desactivado\n","                \n","                running_loss = 0.\n","                \n","                idx = np.random.permutation(self.num_val)\n","\n","                running_loss = 0.\n","                \n","                for i in range(int(self.num_batchs_val)):\n","                    \n","                    idx_batch = idx[i*self.batch_size:(i+1)*self.batch_size]\n","\n","                    out = self.forward(Xval[idx_batch,:])\n","\n","                    loss = self.criterion(out,labelsval[idx_batch])\n","\n","                    running_loss += loss.item() \n","                    \n","                self.valid_loss_during_training.append(running_loss/self.num_batchs_val)    \n","                    \n","                \n","\n","            if(e % self.print_every == 0): \n","\n","                print(f\"Training loss after {e} epochs: {self.loss_during_training[-1]}. Validation loss: {self.valid_loss_during_training[-1]}\")\n"]},{"cell_type":"markdown","id":"c28fafc7","metadata":{"id":"c28fafc7"},"source":["Instanciamos la clase ..."]},{"cell_type":"code","execution_count":null,"id":"65e77091","metadata":{"id":"65e77091"},"outputs":[],"source":["my_MLP = MLP_with_train(dimx=W.shape[1],hidden1=10,hidden2=5,nlabels=2,prob=0.3)\n"]},{"cell_type":"code","execution_count":null,"id":"5195eb15","metadata":{"id":"5195eb15"},"outputs":[],"source":["my_MLP.fit(torch.Tensor(W_[idx_train,:]),torch.Tensor(y_train),torch.Tensor(W_[idx_val,:]),torch.Tensor(y_val),\n","           epochs=120,print_every=10)\n","\n"]},{"cell_type":"markdown","id":"967b60a5","metadata":{"id":"967b60a5"},"source":["Visualizaremos la función de coste en ambos casos ..."]},{"cell_type":"code","execution_count":null,"id":"61fc3002","metadata":{"id":"61fc3002"},"outputs":[],"source":["plt.plot(my_MLP.loss_during_training,label='Train_loss')\n","plt.plot(my_MLP.valid_loss_during_training,label='Valid_loss')\n","plt.grid()\n","plt.legend()"]},{"cell_type":"markdown","id":"a2be9d91","metadata":{"id":"a2be9d91"},"source":["Calculamos la salida para el conjunto de test (log-probabilidad por dato), y evaluamos métricas ..."]},{"cell_type":"code","execution_count":null,"id":"0c87835f","metadata":{"tags":[],"id":"0c87835f"},"outputs":[],"source":["out_test = my_MLP.forward(torch.Tensor(W_[idx_test,:])).detach().numpy()"]},{"cell_type":"code","execution_count":null,"id":"a0ef6ae9","metadata":{"id":"a0ef6ae9"},"outputs":[],"source":["# Accuracy en test\n","\n","np.sum(np.argmax(out_test,1)==y_test)/np.shape(y_test)[0]"]},{"cell_type":"code","execution_count":null,"id":"c92e4634","metadata":{"id":"c92e4634"},"outputs":[],"source":["out_train = my_MLP.forward(torch.Tensor(W_[idx_train,:])).detach().numpy()"]},{"cell_type":"code","execution_count":null,"id":"1da8cb8d","metadata":{"id":"1da8cb8d"},"outputs":[],"source":["# Accuracy en train\n","\n","np.sum(np.argmax(out_train,1)==y_train)/np.shape(y_train)[0]"]},{"cell_type":"markdown","id":"8ed390f8","metadata":{"id":"8ed390f8"},"source":["Curvas ROC y PR ..."]},{"cell_type":"code","execution_count":null,"id":"78b89537","metadata":{"id":"78b89537"},"outputs":[],"source":["fpr4, recall4, thresholds = metrics.roc_curve(y_test, np.exp(out_test)[:,1], pos_label=1) \n","\n","fig,ax = plt.subplots()\n","plt.plot(fpr,recall,lw=2.5,label='Curva ROC TF-IDF')\n","plt.plot(fpr2,recall2,lw=2.5,label='Curva ROC embeddings')\n","plt.plot(fpr3,recall3,lw=2.5,label='Curva ROC embeddings con K-NN')\n","plt.plot(fpr4,recall4,lw=2.5,label='Curva ROC embeddings con MLP')\n","plt.legend(loc=7)\n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('Recall (R)')\n","plt.title('Curva ROC')\n","plt.show()\n","\n","area_roc_embbedings_MLP = metrics.roc_auc_score(y_test, np.exp(out_test)[:,1])\n","\n","print(f\"El área bajo la curva ROC de TF-IDF es {area_roc_tf_idf}\")\n","print(f\"El área bajo la curva ROC de embeddings es {area_roc_embbedings}\")\n","print(f\"El área bajo la curva ROC de embeddings con K-NN es {area_roc_embbedings_knn}\")\n","print(f\"El área bajo la curva ROC de embeddings con MLP es {area_roc_embbedings_MLP}\")"]},{"cell_type":"markdown","id":"40e3da28","metadata":{"id":"40e3da28"},"source":["Podemos comprobar cómo el nuevo clasificador no consigue una gran mejora con respecto a soluciones anteriores. \n"]},{"cell_type":"markdown","id":"69a9f5f6","metadata":{"id":"69a9f5f6"},"source":["## 6. Clasificación RNN + Regresión Logística\n","\n","Finalmente, vamos a comprobar cómo un procesado secuencial de las palabras utilizando una RNN permite una representación vectorial del texto más fiel al contenido, permitiendo una mejora de las prestaciones del clasificador. Tanto es así, que el clasificador será una simple regresión logística sobre el último estado de la RNN.\n","\n","<img src='http://www.tsc.uc3m.es/~olmos/BBVA/RNN.png' width=800 />\n","\n","\n","Primero, vamos a normalizar la longitud de las secuencias de texto. Esto es solo un requisito de entrada para la función RNN de pytorch. Una vez obtengamos la secuencia de estados de la RNN, **utilizaremos el estado resultante tras procesar la última palabra del texto**."]},{"cell_type":"code","execution_count":null,"id":"88038334","metadata":{"id":"88038334"},"outputs":[],"source":["# Calculamos la longitud por texto\n","\n","longitudes = [len(d) for d in norm_docs]\n","\n","# Máxima longitud\n","max_l = np.max(longitudes)"]},{"cell_type":"code","execution_count":null,"id":"6a18779b","metadata":{"id":"6a18779b"},"outputs":[],"source":["max_l"]},{"cell_type":"code","execution_count":null,"id":"0449ea42","metadata":{"id":"0449ea42"},"outputs":[],"source":["# Igualar dataset añadiendo un token \"basura\" (no se tendrá en cuenta)\n","\n","garbage_token = nlp('#') # Token basura --> #\n","\n","norm_docs_eq_length = [norm_docs[d]+[garbage_token]*(max_l-longitudes[d]) for d in range(len(norm_docs))]\n","\n","# documentos train\n","\n","docs_train = [norm_docs_eq_length[d] for d in idx_train]\n","\n","len_train = [longitudes[d] for d in idx_train]\n","\n","# documentos validación\n","\n","docs_val = [norm_docs_eq_length[d] for d in idx_val]\n","\n","len_val = [longitudes[d] for d in idx_val]\n","\n","# documentos test\n","\n","docs_test = [norm_docs_eq_length[d] for d in idx_test]\n","\n","len_test = [longitudes[d] for d in idx_test]"]},{"cell_type":"markdown","id":"bf348b58","metadata":{"id":"bf348b58"},"source":["A continuación creamos la clase RNN. A la hora de construir una RNN debemos especificar los siguientes parámetros\n","\n","* **input_size** - En nuestro caso dimensión de cada word embedding (300)\n","* **hidden_dim** - La dimensión del estado de la LSTM\n","* **n_layers** - Número de **LSTMs apiladas**, tal y como se ilustra en la siguiente figura \n","* **dropout** - Probabilidad de dropout entre capas (sólo si n_layers>1) \n","\n","<img src=\"https://yiyibooks.cn/__src__/wizard/nmt-tut-neubig-2017_20180721165003_deleted/img/6-5.jpg\" width=\"40%\"> \n","\n","Se aconseja ver la documentación oficial de la capa [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) para entender todos sus parámetros:"]},{"cell_type":"code","execution_count":null,"id":"52f81c17","metadata":{"id":"52f81c17"},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(self, input_size, output_size, hidden_dim, n_layers,prob=0.5):\n","        \n","        \n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        \n","        self.input_size = input_size\n","        \n","        # Capa LSTM\n","        # batch_first=True significa que la primera dimensión del tensor de entrada indexa datos distintos\n","        self.rnn = nn.LSTM(input_size, hidden_dim, n_layers, dropout=prob, batch_first=True)\n","        \n","        # last, fully-connected layer\n","        self.fc1 = nn.Linear(hidden_dim, output_size) \n","        \n","        self.logsoftmax = nn.LogSoftmax(dim=1) \n","        \n","        # Capa dropout \n","        \n","        self.dropout = nn.Dropout(p=prob)\n","\n","    def forward(self, x, lengths, h0=None):\n","        \n","        '''\n","        \n","        - x: secuencias de texto, codificadas con word embeddings. Dimensiones (batch_size, seq_length, input_size)\n","        - lengths: la longitud de cada texto (antes de meter el token de relleno). Se usa para mirar el estado correcto\n","          para clasificar.\n","        \n","        Sobre las dimensiones de los tensores de entrada ...:\n","        \n","        - Señal de entrada a RNN tiene dimensiones (batch_size, seq_length, input_size)\n","        - La inicialización del estado de la RNN tiene dimensiones (n_layers, batch_size, hidden_dim).\n","          Si se usa None, se inicializa con ceros.\n","        - La salida de la RNN tiene dimensiones (batch_size, seq_length, hidden_size). \n","          Esta salida es el estado de la RNN a lo largo del tiempo para cada dato \n","\n","        '''\n","        \n","        batch_size = x.size(0) \n","        seq_length = x.size(1) \n","        \n","        # Calculamos la salida de la RNN\n","        # r_out es la secuencia de estados\n","        r_out, _ = self.rnn(x, h0)\n","        \n","        # Usamos el estado correspondiente a procesar la última palabra, antes de meter el relleno.\n","        # Con el reshape pasamos a dimensiones (batch_ize, hidden_dim)\n","        \n","        \n","        aux=torch.stack([r_out[[d],lengths[d]-1,:] for d in range(batch_size)]).reshape([-1,self.hidden_dim])\n","        \n","        # Clasificamos usando una log-softmax\n","        \n","        output = self.logsoftmax(self.fc1(self.dropout(aux)))\n","    \n","        \n","        return output\n"]},{"cell_type":"markdown","id":"ffbea5b0","metadata":{"id":"ffbea5b0"},"source":["Vamos a ilustrar cómo podemos obtener la salida de la red dados nuestros textos. Por ejemplo, obtengamos la salida de la RNN para los tres primeros textos. El primer paso es obtener las **secuencias de word embeddings** de cada uno de ellos ..."]},{"cell_type":"code","execution_count":null,"id":"8dbc4d14","metadata":{"id":"8dbc4d14"},"outputs":[],"source":["idx = [1,2,3]\n","x_input = torch.Tensor([[w.vector for w in norm_docs_eq_length[d]] for d in idx])"]},{"cell_type":"code","execution_count":null,"id":"e1671012","metadata":{"id":"e1671012"},"outputs":[],"source":["x_input.shape"]},{"cell_type":"markdown","id":"a2e7f2d9","metadata":{"id":"a2e7f2d9"},"source":["Instanciamos la clase anterior"]},{"cell_type":"code","execution_count":null,"id":"bbc29063","metadata":{"id":"bbc29063"},"outputs":[],"source":["my_RNN = RNN(300,2,20,1)"]},{"cell_type":"code","execution_count":null,"id":"5cd9aac0","metadata":{"id":"5cd9aac0"},"outputs":[],"source":["o = my_RNN.forward(torch.Tensor(x_input),[longitudes[d] for d in idx])"]},{"cell_type":"code","execution_count":null,"id":"8949f788","metadata":{"id":"8949f788"},"outputs":[],"source":["print(torch.exp(o))"]},{"cell_type":"markdown","id":"7ec5d42b","metadata":{"id":"7ec5d42b"},"source":["Tal y como hicimos anteriormente, extendemos la clase para añadir un método de entrenamiento. El entrenamiento de RNNs puede ser lento, y se recomienda añadir funcionalidades para explotar hardware de tipo GPU. Un ejemplo puede encontrarse en este [tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n","\n","Se han añadido funcionalidades para **salvar la configuración de la red al final de cada época durante el entrenamiento**. Esto nos permitirá recuperar la red que minimiza el error de validación."]},{"cell_type":"code","execution_count":null,"id":"5e2db153","metadata":{"id":"5e2db153"},"outputs":[],"source":["class RNN_with_train(RNN):\n","    \n","    def __init__(self,input_size, output_size, hidden_dim, n_layers,prob=0.0,batch_size=50,lr=0.0005,saved_files='./RNN_sentiment_analysis'):\n","        \n","        super().__init__(input_size, output_size, hidden_dim, n_layers,prob)  \n","        \n","        self.lr = lr # Learning Rate\n","        \n","        self.optim = optim.Adam(self.parameters(), self.lr) # Optimizador\n","        \n","        self.criterion = nn.NLLLoss()               \n","        \n","        self.loss_during_training = [] \n","        \n","        self.valid_loss_during_training = [] \n","        \n","        self.batch_size = batch_size\n","        \n","        self.saved_files = saved_files\n","        \n","        \n","    def predict_proba(self,docs,lengths,Y=0):\n","        \n","        '''\n","        Este método lo usamos para obtener la salida de la red dado un conjunto de documentos. Si se proporciona etiqueta,\n","        obtenemos accuracy.\n","        \n","        - docs: documentos, en formato lista de spacy tokens. Normalizados con igual longitud añadiendo tokens basura\n","        - lengths: longitud real de cada texto.\n","        - Y: Etiquetas\n","        '''\n","        \n","        x_input = torch.Tensor([[w.vector for w in d] for d in docs])\n","        \n","        logprobs = self.forward(x_input,lengths).detach().numpy()\n","            \n","        accuracy = np.sum(np.argmax(logprobs,1)==Y)/np.shape(Y)[0]\n","            \n","        return logprobs,accuracy\n","            \n","        \n","    def fit(self,docs_train,docs_val,Y,Yval,len_train,len_val,epochs=100,print_every=5):\n","        \n","        '''\n","        Método de entrenamiento.\n","        \n","        - docs_train, docs_val: documentos de training/validación (secuencias de spacy tokens). \n","          Normalizados con igual longitud añadiendo tokens basura\n","        - len_train/len_val: longitudes reales\n","        '''\n","        \n","        self.print_every = print_every\n","        \n","        self.epochs=epochs\n","        \n","        # Bucle de optimización\n","        \n","        self.num_train = len(docs_train) # Num datos de entrenamiento\n","        \n","        self.num_batchs = np.floor(self.num_train/self.batch_size) # Numero de batches de training\n","        \n","        self.num_val = len(docs_val) # Num datos de validación\n","        \n","        self.num_batchs_val = np.floor(self.num_val/self.batch_size) # Numero de batches de validación \n","        \n","        labels = torch.Tensor(Y).type(torch.LongTensor) # Etiquetas training\n","        \n","        labelsval = torch.Tensor(Yval).type(torch.LongTensor) # Etiquetas validación\n","        \n","        \n","        for e in range(int(self.epochs)):\n","            \n","            self.train() # Dropout activado\n","            \n","            # Permutación aleatoria datos\n","            \n","            idx = np.random.permutation(self.num_train)\n","            \n","            running_loss = 0.\n","            \n","            for i in range(int(self.num_batchs)):\n","                        \n","                self.optim.zero_grad()  # Ponemos gradientes a cero\n","\n","                # Ínidices de los datos que entran al batch\n","\n","                idx_batch = idx[i*self.batch_size:(i+1)*self.batch_size]\n","                \n","                # Codificamos entradas como secuencias de embeddings\n","                \n","                x_input = torch.Tensor([[w.vector for w in docs_train[d]] for d in idx_batch])\n","                \n","                # Calculamos probabilidades por dato\n","\n","                out = self.forward(x_input,[len_train[d] for d in idx_batch])\n","                \n","                # Evaluamos función de coste\n","\n","                loss = self.criterion(out,labels[idx_batch])\n","                \n","                # Guardamos para monitorizar\n","\n","                running_loss += loss.item()\n","                \n","                # Calculamos gradientes\n","\n","                loss.backward()\n","\n","                # Limitamos el valor de los gradientes para evitar problemas numéricos (típicos en RNNs)\n","                nn.utils.clip_grad_norm_(self.parameters(), 2.0)\n","                \n","                # Iteración SGD\n","                \n","                self.optim.step()\n","                \n","            self.loss_during_training.append(running_loss/self.num_batchs)\n","            \n","            # Guardamos los parámetros del modelo\n","            \n","            torch.save(self.state_dict(), self.saved_files+'_epoch_'+str(e)+'.pth')\n","            \n","            # Repetimos para validación, pero sin evaluar gradientes\n","            \n","            with torch.no_grad(): \n","                \n","                # Pasamos el modelo a modo de evaluación\n","                self.eval()\n","                \n","                running_loss = 0.\n","                \n","                idx = np.random.permutation(self.num_val)\n","\n","                running_loss = 0.\n","                \n","                for i in range(int(self.num_batchs_val)):\n","                    \n","                    idx_batch = idx[i*self.batch_size:(i+1)*self.batch_size] \n","                    \n","                    x_input = torch.Tensor([[w.vector for w in docs_val[d]] for d in idx_batch])\n","\n","                    out = self.forward(x_input,[len_val[d] for d in idx_batch])\n","\n","                    loss = self.criterion(out,labelsval[idx_batch])\n","\n","                    running_loss += loss.item() \n","                    \n","                self.valid_loss_during_training.append(running_loss/self.num_batchs_val)    \n","                    \n","                \n","\n","            if(e % self.print_every == 0): \n","\n","                print(f\"Training loss after {e} epochs: {self.loss_during_training[-1]}. Validation loss: {self.valid_loss_during_training[-1]}\")"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"vOSwLDQkhLEd"},"id":"vOSwLDQkhLEd","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"249a0bf3","metadata":{"id":"249a0bf3"},"source":["Definida la red neuronal y los métodos para su entrenamiento, instaciamos la clase y entrenamos. Utilizamos un estado de la LSTM de 20 dimensiones y una sóla capa LSTM, pero estos parámetro **no se han validado**. HAY MUCHO MARGEN DE MEJORA!"]},{"cell_type":"code","execution_count":null,"id":"4a8e8d98","metadata":{"id":"4a8e8d98"},"outputs":[],"source":["my_RNN = RNN_with_train(input_size=300,output_size=2,hidden_dim=100,prob=0.6,n_layers=1)"]},{"cell_type":"code","execution_count":null,"id":"bc9241ec","metadata":{"id":"bc9241ec"},"outputs":[],"source":["my_RNN.fit(docs_train,docs_val,torch.Tensor(y_train),torch.Tensor(y_val),len_train,len_val,epochs=30,print_every=1)"]},{"cell_type":"markdown","id":"a85b108b","metadata":{"id":"a85b108b"},"source":["Representamos la evolución de la función de coste en entrenamiento/validación ..."]},{"cell_type":"code","execution_count":null,"id":"df7a98ca","metadata":{"id":"df7a98ca"},"outputs":[],"source":["plt.plot(my_RNN.loss_during_training,label='Train_loss')\n","plt.plot(my_RNN.valid_loss_during_training,label='Valid_loss')\n","plt.legend()\n","plt.grid()"]},{"cell_type":"markdown","id":"da23c8cb","metadata":{"id":"da23c8cb"},"source":["Con el siguiente código, recuperamos los parámetros de la red que minimizan el error de validación ..."]},{"cell_type":"code","execution_count":null,"id":"f6657dc4","metadata":{"id":"f6657dc4"},"outputs":[],"source":["# Época en la que se minimiza el error de validación\n","\n","idx_min = np.argsort(my_RNN.valid_loss_during_training)"]},{"cell_type":"code","execution_count":null,"id":"268de5cb","metadata":{"id":"268de5cb"},"outputs":[],"source":["idx_min[0]"]},{"cell_type":"markdown","id":"93ffad6c","metadata":{"id":"93ffad6c"},"source":["Cargamos la red"]},{"cell_type":"code","execution_count":null,"id":"7f5362ec","metadata":{"id":"7f5362ec"},"outputs":[],"source":["state_dict = torch.load(my_RNN.saved_files+'_epoch_'+str(idx_min[0])+'.pth')\n","\n","my_RNN.load_state_dict(state_dict)"]},{"cell_type":"markdown","id":"65d825c5","metadata":{"id":"65d825c5"},"source":["Calculamos el accuracy en test y las métricas ROC y curva PR ..."]},{"cell_type":"code","execution_count":null,"id":"5b716e88","metadata":{"id":"5b716e88"},"outputs":[],"source":["probs,acc = my_RNN.predict_proba(docs_test,len_test,y_test)\n","\n","probs = np.exp(probs)"]},{"cell_type":"code","execution_count":null,"id":"98d9d922","metadata":{"id":"98d9d922"},"outputs":[],"source":["print(acc)"]},{"cell_type":"markdown","id":"1a6bb717","metadata":{"id":"1a6bb717"},"source":["En términos de accuracy superamos a todos los modelos anteriores. "]},{"cell_type":"code","execution_count":null,"id":"ee8d956f","metadata":{"id":"ee8d956f"},"outputs":[],"source":["fpr4, recall4, thresholds = metrics.roc_curve(y_test, probs[:,1], pos_label=1) \n","\n","fig,ax = plt.subplots()\n","plt.plot(fpr,recall,lw=2.5,label='Curva ROC TF-IDF')\n","plt.plot(fpr2,recall2,lw=2.5,label='Curva ROC embeddings')\n","plt.plot(fpr3,recall3,lw=2.5,label='Curva ROC embeddings con K-NN')\n","plt.plot(fpr4,recall4,lw=2.5,label='Curva ROC LSTM')\n","plt.legend(loc=7)\n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('Recall (R)')\n","plt.title('Curva ROC')\n","plt.show()\n","\n","area_roc_tf_idf = metrics.roc_auc_score(y_test, LR_with_CV.predict_proba(C[idx_test,:])[:,1])\n","area_roc_embbedings = metrics.roc_auc_score(y_test, emb_LR_with_CV.predict_proba(W_test)[:,1])\n","area_roc_embbedings_knn = metrics.roc_auc_score(y_test, knn.predict_proba(W_[idx_test,:])[:,1])\n","area_roc_LSTM = metrics.roc_auc_score(y_test, probs[:,1])\n","\n","print(f\"El área bajo la curva ROC de TF-IDF es {area_roc_tf_idf}\")\n","print(f\"El área bajo la curva ROC de embeddings es {area_roc_embbedings}\")\n","print(f\"El área bajo la curva ROC de embeddings con K-NN es {area_roc_embbedings_knn}\")\n","print(f\"El área bajo la curva ROC de LSTM es {area_roc_LSTM}\")"]},{"cell_type":"code","execution_count":null,"id":"a2dc42aa","metadata":{"id":"a2dc42aa"},"outputs":[],"source":["P4, R4, thresholds = metrics.precision_recall_curve(y_test, probs[:,1], pos_label=1) \n","\n","fig,ax = plt.subplots()\n","plt.plot(R,P,lw=2.5,label='Curva PR TF-IDF')\n","plt.plot(R2,P2,lw=2.5,label='Curva PR embeddings')\n","plt.plot(R3,P3,lw=2.5,label='Curva PR embeddings K-NN')\n","plt.plot(R4,P4,lw=2.5,label='Curva PR LSTM+embeddings')\n","plt.legend(loc=3)\n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.xlabel('Recall (R)')\n","plt.ylabel('Precision (P)')\n","plt.title('Curva PR')\n","plt.show()\n","\n","area_pr_tf_idf = metrics.average_precision_score(y_test, LR_with_CV.predict_proba(C[idx_test,:])[:,1])\n","area_pr_embbedings = metrics.average_precision_score(y_test, emb_LR_with_CV.predict_proba(W_test)[:,1])\n","area_pr_embbedings_knn = metrics.average_precision_score(y_test, knn.predict_proba(W_[idx_test,:])[:,1], pos_label=1) \n","area_pr_LSTM = metrics.average_precision_score(y_test, probs[:,1], pos_label=1) \n","\n","print(f\"El área bajo la curva PR de TF-IDF es {area_pr_tf_idf}\")\n","print(f\"El área bajo la curva PR de embeddings es {area_pr_embbedings}\")\n","print(f\"El área bajo la curva PR de embeddings con k-NN es {area_pr_embbedings_knn}\")\n","print(f\"El área bajo la curva PR de embeddings con LSTM {area_pr_LSTM}\")"]},{"cell_type":"markdown","id":"b202c64b","metadata":{"id":"b202c64b"},"source":["La mejora es evidente sobre todo a la vista de la curva PR.\n","\n","Es interesante mostrar ejemplos de frases en las que el mejor método hasta ahora (k-NN con medias de word embeddings) falla y el método con LSTMs no. "]},{"cell_type":"code","execution_count":null,"id":"e3306633","metadata":{"id":"e3306633"},"outputs":[],"source":["errores_LSTM = np.where(np.argmax(probs,1)!=y_test)[0]"]},{"cell_type":"code","execution_count":null,"id":"44016296","metadata":{"id":"44016296"},"outputs":[],"source":["len(errores_LSTM)"]},{"cell_type":"code","execution_count":null,"id":"719f07f1","metadata":{"id":"719f07f1"},"outputs":[],"source":["len(errores_k_NN)"]},{"cell_type":"markdown","id":"22062e0c","metadata":{"id":"22062e0c"},"source":["Podemos ver cómo para un umbral de decisión de 0.5, con LSTMs tenemos 26 fallos menos que con un k-NN con medias de word embeddings."]},{"cell_type":"code","execution_count":null,"id":"1bbb1490","metadata":{"id":"1bbb1490"},"outputs":[],"source":["errores_LSTM"]},{"cell_type":"code","execution_count":null,"id":"b1b593cf","metadata":{"id":"b1b593cf"},"outputs":[],"source":["errores_k_NN"]},{"cell_type":"markdown","id":"4d0e423f","metadata":{"id":"4d0e423f"},"source":["Imprimamos una de las frases en las que k-NN falla ..."]},{"cell_type":"code","execution_count":null,"id":"a98540b9","metadata":{"id":"a98540b9"},"outputs":[],"source":["\n","i = 105\n","\n","df.iloc[idx_test[i]]['Phrase']"]},{"cell_type":"code","execution_count":null,"id":"208c9b73","metadata":{"id":"208c9b73"},"outputs":[],"source":["y_test[i]"]},{"cell_type":"markdown","id":"b9b979a2","metadata":{"id":"b9b979a2"},"source":["La anterior frase tiene etiqueta 0 (positiva/neutral). **Sin embargo k-NN falla ya que no considera el orden de las palabras y, desde el punto de vista de un BoW, hay muchas palabras negativas!!**"]},{"cell_type":"code","execution_count":null,"id":"27f5b6db","metadata":{"id":"27f5b6db"},"outputs":[],"source":["knn.predict_proba(W_[idx_test[i],:].reshape([1,-1]))"]},{"cell_type":"markdown","id":"e7ea7b45","metadata":{"id":"e7ea7b45"},"source":["Sin embargo, el procesado secuencial de LSTM sí permite clasificar correctamente esta frase ..."]},{"cell_type":"code","execution_count":null,"id":"7fd3798c","metadata":{"id":"7fd3798c"},"outputs":[],"source":["probs[i]"]},{"cell_type":"markdown","id":"5f70b7eb","metadata":{"id":"5f70b7eb"},"source":["## 7. Clasificación multi-clase\n","\n","Finalmente, vamos a ilustrar cómo en el caso de clasificación multi-clase, usando las tres etiquetas originales, de nuevo WE + LSTMs + Regresión Logística alcanza el mejor resultado ..."]},{"cell_type":"code","execution_count":null,"id":"9b27ade1","metadata":{"id":"9b27ade1"},"outputs":[],"source":["# Separamos train de test\n","idx_train, idx_test, y_train_multiclass, y_test_multiclass = train_test_split(idx_data, labels, test_size=0.2, random_state=0)\n","\n","# Separamos train de val\n","idx_train, idx_val, y_train_multiclass, y_val_multiclass = train_test_split(idx_train, y_train_multiclass, test_size=0.2, random_state=0)\n","\n","rango_C = np.logspace(-3, 3, 20)  # Rango C en escala logarítmica (base 10). Esto es, 20 puntos desde 10^3, a 10^3.\n","diccionario_parametros = [{'C': rango_C}]   \n","nfold = 10 # Número de particiones train/validación\n","\n","\"\"\" Ajusto C por validación cruzada\n","El optimizador por defecto ('lbfgs') no acepta regularización l1. \n","Usamos 'liblinear' siguiendo las recomendaciones de la librería.\n","\"\"\"\n","LR_with_CV  = GridSearchCV(estimator=LR(penalty='l1',max_iter=1e08,solver='liblinear'),\n","                                  param_grid=diccionario_parametros,cv=nfold)\n","# Entrenar el modelo\n","LR_with_CV.fit(C[idx_train,:],y_train_multiclass)   \n","\n","print(\"El mejor parámetro C es {0:.2f}\".format(LR_with_CV.best_params_['C']))\n","                        \n","# Score de clasificación en train/test\n","accuracy_train = LR_with_CV.score(C[idx_train,:],y_train_multiclass)   \n","accuracy_test = LR_with_CV.score(C[idx_test,:],y_test_multiclass)  \n","\n","print(\"Accuracy train {0:.2f}%. Accuracy test {1:.2f}%\\n\".format(accuracy_train*100, accuracy_test*100))"]},{"cell_type":"code","execution_count":null,"id":"3a1416ed","metadata":{"id":"3a1416ed"},"outputs":[],"source":["# Entrenamiento k-NN con validación de vecinos y ponderación de distancias\n","K_max = 15\n","rango_K = np.arange(1, K_max+1)\n","nfold = 10\n","# Define un diccionario con el nombre de los parámetros a explorar como claves y el rango como valores\n","diccionario_parametros = [{'n_neighbors': rango_K,'weights':['uniform','distance']}]\n","\n","# Validación cruzada con GridSearchCV\n","knn = GridSearchCV(estimator=KNeighborsClassifier( ), param_grid=diccionario_parametros,cv=nfold)\n","# Entrenamiento\n","knn.fit(W_[idx_train,:],y_train_multiclass)\n","# Test\n","accuracy_train_knn = knn.score(W_[idx_train,:],y_train_multiclass)\n","accuracy_test_knn = knn.score(W_[idx_test,:],y_test_multiclass)\n","\n","print(\"El número de vecinos seleccionado es k={0:d}\".format(knn.best_params_['n_neighbors']))\n","print(\"Accuracy train {0:.2f}%. Accuracy test {1:.2f}%\\n\".format(accuracy_train_knn*100, accuracy_test_knn*100))"]},{"cell_type":"code","execution_count":null,"id":"0d5f03be","metadata":{"id":"0d5f03be"},"outputs":[],"source":["from sklearn.metrics import plot_confusion_matrix\n","\n","plot_confusion_matrix(LR_with_CV, C[idx_test,:], y_test_multiclass,normalize='true',display_labels=['Neutral','Negative','Positive'])\n","plt.title('Matrix confusión, codificación TF-IDF')\n","\n","plot_confusion_matrix(knn, W_[idx_test,:], y_test_multiclass,normalize='true',display_labels=['Neutral','Negative','Positive'])\n","plt.title('Matrix confusión, codificación embeddings')"]},{"cell_type":"code","execution_count":null,"id":"3fcd9df7","metadata":{"id":"3fcd9df7"},"outputs":[],"source":["docs_train = [norm_docs_eq_length[d] for d in idx_train]\n","\n","len_train = [longitudes[d] for d in idx_train]\n","\n","docs_val = [norm_docs_eq_length[d] for d in idx_val]\n","\n","len_val = [longitudes[d] for d in idx_val]\n","\n","docs_test = [norm_docs_eq_length[d] for d in idx_test]\n","\n","len_test = [longitudes[d] for d in idx_test]"]},{"cell_type":"markdown","id":"511e06db","metadata":{"id":"511e06db"},"source":["Reultizamos la clase `RNN_with_train` que construímos anteriormente, especificando ahora `output_size=3`, puesto que tenemos tres etiquetas ..."]},{"cell_type":"code","execution_count":null,"id":"4497aad0","metadata":{"id":"4497aad0"},"outputs":[],"source":["my_RNN_3_class = RNN_with_train(input_size=300,output_size=3,hidden_dim=100,n_layers=1,saved_files='./RNN_sentiment_analysis_multiclass',prob=0.6)"]},{"cell_type":"code","execution_count":null,"id":"caad19f8","metadata":{"id":"caad19f8"},"outputs":[],"source":["my_RNN_3_class.fit(docs_train,docs_val,torch.Tensor(y_train_multiclass),torch.Tensor(y_val_multiclass),len_train,len_val,epochs=20,print_every=2)"]},{"cell_type":"code","execution_count":null,"id":"9f545ab5","metadata":{"id":"9f545ab5"},"outputs":[],"source":["plt.plot(my_RNN_3_class.loss_during_training,label='Train_loss')\n","plt.plot(my_RNN_3_class.valid_loss_during_training,label='Valid_loss')\n","plt.grid()\n","plt.legend()"]},{"cell_type":"markdown","id":"fcbac848","metadata":{"id":"fcbac848"},"source":["Recuperamos la red en el mínimo de la función de coste en validación ..."]},{"cell_type":"code","execution_count":null,"id":"b55a4a51","metadata":{"id":"b55a4a51"},"outputs":[],"source":["idx_min = np.argsort(my_RNN_3_class.valid_loss_during_training)"]},{"cell_type":"code","execution_count":null,"id":"ebd65398","metadata":{"id":"ebd65398"},"outputs":[],"source":["idx_min[0]"]},{"cell_type":"code","execution_count":null,"id":"7ed2fc34","metadata":{"id":"7ed2fc34"},"outputs":[],"source":["state_dict = torch.load(my_RNN_3_class.saved_files+'_epoch_'+str(idx_min[0])+'.pth')\n","\n","my_RNN_3_class.load_state_dict(state_dict)"]},{"cell_type":"code","execution_count":null,"id":"e74ee63a","metadata":{"id":"e74ee63a"},"outputs":[],"source":["probs,acc = my_RNN_3_class.predict_proba(docs_test,len_test,y_test_multiclass)"]},{"cell_type":"code","execution_count":null,"id":"d211e932","metadata":{"id":"d211e932"},"outputs":[],"source":["print(acc)"]},{"cell_type":"code","execution_count":null,"id":"abd8e142","metadata":{"id":"abd8e142"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","\n","cm = confusion_matrix(y_test_multiclass, np.argmax(probs,1))\n","\n","cm /np.sum(cm,1)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}