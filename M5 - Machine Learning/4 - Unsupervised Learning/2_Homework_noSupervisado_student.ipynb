{"cells":[{"cell_type":"markdown","source":["# Aprendizaje no supervisado: homework\n","\n","\n","------------------------------------------------------\n","\n","\n","### Data Science and Machine Learning\n","\n","#### Febrero 2023\n","\n","**Aurora Cobo Aguilera**\n","\n","**The Valley**\n","\n","------------------------------------------------------"],"metadata":{"id":"HHVVD1ZYughv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3kyvxpK3ubN"},"outputs":[],"source":["from IPython.core.display import Image, display"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOGgPkN63ubT"},"outputs":[],"source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina' "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zh_LPudm3ubU"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from matplotlib import rc\n","import numpy as np\n","import pandas as pd\n","\n","\n","# Configuración de las figuras matplotlib\n","plt.rcParams['figure.figsize'] = [8, 6]\n","plt.rcParams.update({'font.size': 8})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GptcAwPe3ubV"},"outputs":[],"source":["def draw_vector(v0, v1, ax=None):\n","    ax = ax or plt.gca()\n","    arrowprops=dict(arrowstyle='->',\n","                    linewidth=2,\n","                    shrinkA=0, shrinkB=0,color='r')\n","    ax.annotate('', v1, v0, arrowprops=arrowprops)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9zWBPZZ3ubV"},"outputs":[],"source":["from scipy.stats import multivariate_normal, norm\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression as LR\n","\n","from warnings import filterwarnings\n","filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"1gLhTXqY3ubX"},"source":["# Ejercicio 1. K-means y GMMs sobre la base de datos Cars93"]},{"cell_type":"markdown","metadata":{"id":"yLQyAljm3ubY"},"source":["En este ejercicio vamos a estudiar los grupos que K-means encuentra en la base de datos Cars93 vista en la sesión anterior."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xdWG7MPW3ubZ"},"outputs":[],"source":["car_data = pd.read_csv('Cars93.csv')\n","\n","car_data.head(10)"]},{"cell_type":"code","source":["car_data[\"Type\"].unique()"],"metadata":{"id":"Cbz2MeWy6VIe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6BV5lNat3uba"},"source":["> **Ejercicio 1.1**: Cree una nueva columna \"Type_Codes\" donde codifique la columna \"Type\" con números enteros"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FY44PkLl3ubb"},"outputs":[],"source":["#<SOL>\n","\n","#</SOL>"]},{"cell_type":"markdown","metadata":{"id":"L_EyjmEG3ubb"},"source":["> **Ejercicio 1.2**: Utilizando únicamente las variables 'Price', 'MPG.highway', 'MPG.city', 'Horsepower', 'Fuel.tank.capacity', 'Passengers', 'Weight', 'Length' y 'RPM'  represente la función de coste de K-means y el coeficiente de silueta para un número de grupos entre 3 y 10. No olvide normalizar los datos previamente.\n",">\n","> A la vista de los resultado, ¿qué valor de K considera razonable?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYDUtLzn3ubc"},"outputs":[],"source":["#<SOL>\n","\n","#</SOL>"]},{"cell_type":"markdown","metadata":{"id":"ZSAmplNt3ubc"},"source":["> **Ejercicio 1.3:** \n","> - Usando el valor de K anterior, represente para los datos en cada uno de los grupos el histograma de las etiquetas \"Type_Codes\". Describa si a la vista de los resultados hay una correlación clara entre grupos y etiquetas.\n",">\n","> - Muestre en un Dataframe de pandas los K centroides debidamente desnormalizados usando el nombre adecuado para las columnas. Discuta la interpretabilidad de los resultados.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FC7jsFPW3ubc"},"outputs":[],"source":["#<SOL>\n","\n","#</SOL>"]},{"cell_type":"markdown","metadata":{"id":"V9goG9Jd3ubd"},"source":["> **Ejercicio 1.4**: Repita el ejercicio anterior usando un GMM. Determine el número de componentes usando el criterio BIC. Utilice un número de reinicializaciones en el GMM suficientemente alto (al menos 100).\n",">\n","> Usando la probabilidad de pertenencia a cada grupo, dibuje el histograma de la [entropia](https://es.wikipedia.org/wiki/Entrop%C3%ADa). La entropía es una medida de la incertidumbre de una distribución de probabilidad. La entropía de una distribución de probabilidad discreta es máxima si la probabilidad es uniforme en todas las categorías. Por otra parte, la entropía es 0 si la probabilidad es 1.0 para una de las categorías y cero para el resto (no hay incertidumbre). Discuta los resultados obtenidos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4RfgN283ubd"},"outputs":[],"source":["#<SOL>\n","\n","#</SOL>"]},{"cell_type":"markdown","metadata":{"id":"2B5EcfrP3ube"},"source":["# Ejercicio 2\n","\n","En este segundo ejercicio vamos a aplicar PCA a una base de datos de caras de personas en blanco y negro. Este conjunto de datos consta de diez imágenes  de 40 sujetos distintos. Para algunos sujetos, las imágenes se tomaron en diferentes momentos, variando la iluminación, las expresiones faciales (ojos abiertos / cerrados, sonriendo / no sonriendo) y con distintos detalles faciales (gafas / sin gafas).\n","    \n","El siguiente código incluye las líneas para descargar este conjunto de datos. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTVAgscj3ubf"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.datasets import fetch_olivetti_faces \n","from sklearn.preprocessing import label_binarize\n","from sklearn.preprocessing import StandardScaler\n","\n","###############################################################################\n","olivetti_people = fetch_olivetti_faces()\n","\n","# Dimensiones de las imágenes\n","n_samples, h, w = olivetti_people.images.shape\n","\n","# Imagenes\n","X = olivetti_people.data\n","n_features = X.shape[1]\n","\n","# Etiquetas\n","Y = olivetti_people.target\n","n_classes = np.unique(Y).shape[0]\n","\n","print(\"Información dataset:\")\n","print(\"Número de pixeles por imágen: %d\" % n_features)\n","print(\"Número de clases: %d\" % n_classes)\n","\n","###############################################################################\n","\n","np.random.seed(1)\n","X_train_a, X_test_a, Y_train, Y_test = train_test_split(X, Y, test_size=0.25) \n","\n","# Normalizamos los datos\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train_a)\n","X_test = scaler.transform(X_test_a)\n","\n","\n","print(\"Numero de datos de entrenamiento: %d\" % X_train.shape[0])\n","print(\"Numero de datos de test: %d\" % X_test.shape[0])"]},{"cell_type":"markdown","metadata":{"id":"2GcWnX573ubf"},"source":["Usamos el siguiente código para representar imágenes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aQhaLRJ-3ubg"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","def plot_gallery(images, titles, h, w, n_row=4, n_col=10):\n","    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n","    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n","    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n","    for i in range(images.shape[0]):\n","        plt.subplot(n_row, n_col, i + 1)\n","        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n","        plt.colorbar()\n","        plt.title(titles[i], size=12)\n","        plt.xticks(())\n","        plt.yticks(())"]},{"cell_type":"markdown","metadata":{"id":"IfzRSCla3ubg"},"source":["Por ejemplo, vamos a representar una imagen de cada clase."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPM4p4Cu3ubg"},"outputs":[],"source":["titles = ['class '+str(c) for c in range(n_classes)]\n","ind_faces = [np.where(Y_train == c)[0][0] for c in range(n_classes)]\n","\n","plot_gallery(X_train_a[ind_faces,:], titles, h, w)"]},{"cell_type":"markdown","metadata":{"id":"OoOE-ONP3ubh"},"source":["> **Ejercicio 2.1:** Utilizando PCA, reduzca el conjunto de datos de entrenamiento y test a 100 dimensiones por imagen. Guarde los resultados en `X_train_pca` y `X_test_pca` respectivamente."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"lj14W3gt3ubh","executionInfo":{"status":"ok","timestamp":1677239140702,"user_tz":-60,"elapsed":6,"user":{"displayName":"AURORA COBO AGUILERA","userId":"02417368943911432830"}}},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","N_feat_max=100\n","\n","#<SOL>\n","\n","#</SOL>"]},{"cell_type":"markdown","metadata":{"id":"uZygMZS_3ubh"},"source":["El análisis de las componentes principales (autovectores) en problemas de detección de rostros es bastante común, de hecho, se conocen como **eigenfaces**. Tenga en cuenta que cada pixel de cada imagen se va a aproximar como una combinación lineal de los pixeles correspondientes de las caras propias.\n","\n","Por otro lado, la varianza explicada (**autovalores**) nos permite conocer la importancia de cada componente.  Analizando los valores propios podemos saber qué componentes son los más relevantes. \n","\n","> **Ejercicio 2.2:** Complete el siguiente código para mostrar las 20 eigenfaces más relevantes."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"T5OTQYdL3ubi"},"outputs":[],"source":["# Salvamos las componentes principales en la variable eigenfaces\n","\n","eigenfaces =  #<SOL>\n","# Dibujas las eigenfaces\n","n_eigenfaces = 20\n","titles = ['Eigenface '+str(num) for num in range(n_eigenfaces)]\n","eigenfaces = eigenfaces.reshape((N_feat_max, h, w))\n","plot_gallery(eigenfaces[:n_eigenfaces,:,:], titles, h, w, n_row=2, n_col=10)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"78oaIlUJ3ubi"},"source":["A continuación vamos a comprobar cómo evoluciona el error de un k-NN a medida que utilizamos más dimensiones de la proyección PCA de cada imagen.\n","\n","> **Ejercicio 2.3**: Complete el siguiente código"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kT0O4YCz3ubi"},"outputs":[],"source":["from sklearn import neighbors\n","\n","\n","def kNN_accuracy_evolution(X_train_t, Y_train, X_test_t, Y_test,K_max):\n","    \"\"\"Compute the accuracy of training, validation and test data for different the number of features\n","\n","    Args:\n","        X_train_t (numpy dnarray): training data projected in the new feature space (number data x number dimensions).\n","        Y_train (numpy dnarray): labels of the training data (number data x 1).\n","        X_val_t (numpy dnarray): validation data projected in the new feature space (number data x number dimensions).\n","        Y_val (numpy dnarray): labels of the validation data (number data x 1).\n","        X_test_t (numpy dnarray): test data projected in the new feature space (number data x number dimensions).\n","        Y_test (numpy dnarray): labels of the test data (number data x 1).                                     \n","   \n","    \"\"\"\n","    \n","    n_fold = 5\n","    \n","    # Fijamos por validación cruzada tanto el número de vecinos como  la ponderación\n","    diccionario_parametros =  #<SOL>\n","    \n","    # Definimos el clasificador k-NN y la validación\n","    clf =  #<SOL>\n","    \n","    acc_tr = []\n","    acc_test = []\n","    for i in range(1,X_train_t.shape[1]):\n","        # Entrenamiento k-NN\n","        #<SOL>\n","        \n","        #</SOL>\n","\n","        # Calcular el número de etiquetas correctas en train y test\n","        acc_tr.append(#<SOL>\n","        acc_test.append(#<SOL>\n","\n","    return np.array(acc_tr), np.array(acc_test)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQhVumlU3ubj"},"outputs":[],"source":["[acc_tr, acc_test] = kNN_accuracy_evolution(X_train_pca, Y_train, X_test_pca, Y_test,K_max=5)\n","\n","plt.figure()\n","plt.plot(acc_tr, \"b-s\", label=\"train\")\n","plt.plot(acc_test, \"r-s\", label=\"test\")\n","plt.xlabel(\"Numero K de componentes principales\")\n","plt.ylabel(\"\\% de acierto\")\n","plt.legend(['Training', 'Test'], loc = 4)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WPa4QV2m3ubj"},"source":["> **Ejercicio 2.4**: Complete el siguiente código, en se muestra la reconstrucción de una imagen usando su proyección PCA a medida que aumentamos el número $K$ de componentes principales."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-B3-IYo3ubj"},"outputs":[],"source":["subject = 27  # Podeis jugar con distintas imágenes para ver el resultado\n","\n","n_reconst_list = [5, 10, 25, 50, 60, 70, 80, 90, 100] # Número de componentes PCA\n","\n","Reconstruccion = np.empty((len(n_reconst_list), 4096)) \n","\n","for ii,n_comp in enumerate(n_reconst_list):\n","    # Reconstrucción de la imagen subjet usando n_comp componentes principales\n","    Reconstruccion[ii,:] =  pca._components[] @ Z[:, :n_comp].T\n","    \n","    \n","# Imagen original\n","Reconstruccion[-1,:] = #<SOL>\n","titles = ['K = '+str(num) for num in n_reconst_list] + ['Original']\n","Reco = Reconstruccion.reshape((len(n_reconst_list), h, w))\n","\n","plot_gallery(Reco, titles, h, w, n_row=6, n_col=10)\n"]},{"cell_type":"markdown","metadata":{"id":"YpHO21fs3ubj"},"source":["> **Ejercicio 2.5**: Entrene un GMM para encontrar grupos de  caras utilizando su proyección PCA con $K=20$ componentes principales. Valide el número de grupos usando la métrica BIC. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYgQSPNN3ubj"},"outputs":[],"source":["#<SOL>\n","\n","#</SOL>"]},{"cell_type":"markdown","metadata":{"id":"bW17c0Fl3ubj"},"source":["> **Ejercicio 2.6**: Finalmente, el siguiente código imprime 5 imágenes por cada grupo. A la vista de los resultados, discuta la interpretabilidad de los grupos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrvTjBMY3ubk"},"outputs":[],"source":["K = #<SOL>\n","\n","titles = ['Grupo '+str(c) for c in range(K)]\n","    \n","for n in range(5):\n","\n","    ind_faces = [np.where(y_gmm == c)[0][n] for c in range(K)] # y_gmm es la predicción de grupo por dato\n","\n","    plot_gallery(X_train[ind_faces,:], titles, h, w)\n"]},{"cell_type":"markdown","metadata":{"id":"ONsuZboe3ubk"},"source":["# Ejercicio 3\n","\n","En este último ejercicio, vamos a comprobar cómo podemos usar un GMM para detectar datos anormalmente poco probables en la distribución aprendida de los datos. Éstos pueden considerarse como puntos anómalos (outliers).\n","\n","Vamos a generar un datatset sintético en el que introduciremos 50 puntos que siguen otra distribución, dibujados en amarillo en la figura. A la vista de los datos obtenidos, observad que hay puntos amarillos claramente fuera del soporte de los 4 grupos mayoritarios, y son estos los que consideraremos como anómalos y los que van a ser fácilmente detectables usando un GMM.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hi7O30sq3ubk"},"outputs":[],"source":["from sklearn.datasets import make_blobs\n","\n","X, y_true = make_blobs(n_samples=400, centers=4,\n","                       cluster_std=0.60, random_state=0)\n","\n","rng = np.random.RandomState(13)\n","X_stretched = np.dot(X, rng.randn(2, 2))\n","\n","# Introducimos 50 outliers\n","center_outliers = np.array([0,2]).reshape([1,-1])\n","X_outlier,_ = make_blobs(n_samples=50,centers=center_outliers, cluster_std=2,random_state=123)\n","X_out_example = np.vstack([X_stretched,X_outlier])\n","y_outlier = np.zeros([X_out_example.shape[0],])\n","y_outlier[-50:] = 1 # Vector binario que vale 1 en los outliers introducidos artificialmente\n","\n","fig, ax = plt.subplots(figsize=(7, 7))\n","ax.scatter(X_out_example[:, 0], X_out_example[:, 1], c=y_outlier, s=40, cmap='viridis')\n","ax.set_xlabel('$x_1$')\n","ax.set_ylabel('$x_2$')\n","ax.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n"]},{"cell_type":"markdown","metadata":{"id":"EeNbAHKx3ubl"},"source":["> **Ejercicio:** Entrene un GMM sobre `X_out_example` validando el número de grupos usando BIC. Usando el método `gmm.score_samples(X)` calcule la (log)-probabilidad de cada dato. Calcule cuántos de los 50 puntos introducidos serán detectados si consideramos anomalos al 10% de datos menos probables de acuerdo al GMM. Represente dichos puntos sobre el dataset original."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CqbqIUhd3ubl"},"outputs":[],"source":["#<SOL>\n","\n","#</SOL>"]},{"cell_type":"code","source":["#<SOL>\n","\n","#</SOL>"],"metadata":{"id":"MpLEAwwySSVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gecIM9t43ubm"},"outputs":[],"source":["log_probs = #<SOL>\n","\n","frac = 0.1\n","idx_outliers = np.argsort(log_probs)[:int(np.round(X_out_example.shape[0]*frac))]\n","\n","fig, ax = plt.subplots(figsize=(7, 7))\n","\n","ax.scatter(X_out_example[:, 0], X_out_example[:, 1], c=y_outlier, s=40, cmap='viridis')\n","ax.scatter(X_out_example[idx_outliers, 0], X_out_example[idx_outliers, 1], marker=\"o\",facecolor=\"none\",edgecolor=\"r\",s=70, label='Outlier estimados')\n","plt.legend()\n","ax.set_xlabel('$x_1$')\n","ax.set_ylabel('$x_2$')\n","ax.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)"]},{"cell_type":"code","source":["print(\"Hemos detectado {0:d} puntos amarillos como anómalos\".format(np.sum(y_outlier[idx_outliers]).astype(np.int)))"],"metadata":{"id":"-66LJsFk2Dv1"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}